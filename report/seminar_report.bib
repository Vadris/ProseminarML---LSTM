@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle=NIPS,
  year={2017}
}

@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  year={1997},
  volume={9},
  number={8},
  pages={1735–1780}
}

@misc{keras-lstm,
    author={keras-team},
    title={lstm.py},
    howpublished={\url{https://github.com/keras-team/keras/blob/master/keras/src/layers/rnn/lstm.py}},
    note={Accessed: June 2025},
    year={2025}
}

@misc{pytorch-rnn,
    author={Sean Robertson},
    title={NLP From Scratch: Classifying Names with a Character-Level RNN},
    howpublished={\url{https://docs.pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html}},
    note={Accessed: June 2025},
    year={2024}
}
@misc{pytorch-lstm,
    author={noplaxochia},
    title={LSTM from scratch},
    howpublished={\url{https://medium.com/@wangdk93/lstm-from-scratch-c8b4baf06a8b}},
    note={Accessed: June 2025},
    year={2024}
}

@article{training-graphs,
author = {Ainsworth, Mark and Shin, Yeonjong},
title = {Plateau Phenomenon in Gradient Descent Training of RELU Networks: Explanation, Quantification, and Avoidance},
journal = {SIAM Journal on Scientific Computing},
volume = {43},
number = {5},
pages = {A3438-A3468},
year = {2021},
doi = {10.1137/20M1353010},
URL = { https://doi.org/10.1137/20M1353010},
eprint = { https://doi.org/10.1137/20M1353010 }
}

@InProceedings{rizakisFPGAlstm,
author="Rizakis, Michalis
and Venieris, Stylianos I.
and Kouris, Alexandros
and Bouganis, Christos-Savvas",
editor="Voros, Nikolaos
and Huebner, Michael
and Keramidas, Georgios
and Goehringer, Diana
and Antonopoulos, Christos
and Diniz, Pedro C.",
title="Approximate FPGA-Based LSTMs Under Computation Time Constraints",
booktitle="Applied Reconfigurable Computing. Architectures, Tools, and Applications",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="3--15",
abstract="Recurrent Neural Networks, with the prominence of Long Short-Term Memory (LSTM) networks, have demonstrated state-of-the-art accuracy in several emerging Artificial Intelligence tasks. Nevertheless, the highest performing LSTM models are becoming increasingly demanding in terms of computational and memory load. At the same time, emerging latency-sensitive applications including mobile robots and autonomous vehicles often operate under stringent computation time constraints. In this paper, we address the challenge of deploying computationally demanding LSTMs at a constrained time budget by introducing an approximate computing scheme that combines iterative low-rank compression and pruning, along with a novel FPGA-based LSTM architecture. Combined in an end-to-end framework, the approximation method parameters are optimised and the architecture is configured to address the problem of high-performance LSTM execution in time-constrained applications. Quantitative evaluation on a real-life image captioning application indicates that the proposed system required up to 6.5{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}less time to achieve the same application-level accuracy compared to a baseline method, while achieving an average of 25{\$}{\$}{\backslash}times {\$}{\$}{\texttimes}higher accuracy under the same computation time constraints.",
isbn="978-3-319-78890-6"
}

@InProceedings{pascanu2013rnntraining,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/pascanu13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/pascanu13.html},
  abstract = 	 {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  }
}

@InProceedings{zheng2018scalability,
  author={Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Benchmarking and Analyzing Deep Neural Network Training}, 
  year={2018},
  volume={},
  number={},
  pages={88-100},
  keywords={Training;Benchmark testing;Tools;Graphics processing units;Computational modeling;Speech recognition;Hardware},
  doi={10.1109/IISWC.2018.8573476}}
os Alamitos, CA, USA},
month =Jun}

@InProceedings {zheng2020scalability,
author = { Zheng, Bojian and Vijaykumar, Nandita and Pekhimenko, Gennady },
booktitle = { 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) },
title = {{ Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training }},
year = {2020},
volume = {},
ISSN = {},
pages = {1089-1102},
abstract = { The Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNNs) are a popular class of machine learning models for analyzing sequential data. Their training on modern GPUs, however, is limited by the GPU memory capacity. Our profiling results of the LSTM RNN-based Neural Machine Translation (NMT) model reveal that feature maps of the attention and RNN layers form the memory bottleneck, and runtime is unevenly distributed across different layers when training on GPUs. Based on these two observations, we propose to recompute the feature maps of the attention and RNN layers rather than stashing them persistently in the GPU memory. While the idea of feature map recomputation has been considered before, existing solutions fail to deliver satisfactory footprint reduction, as they do not address two key challenges. For each feature map recomputation to be efficient, its effect on (1) the total memory footprint, and (2) the total execution time has to be carefully estimated. To this end, we propose Echo, a new compiler-based optimization scheme that addresses the first challenge with a practical mechanism that estimates the memory benefits of recomputation over the entire computation graph, and the second challenge by non-conservatively estimating the recomputation runtime overhead leveraging layer specifics. Echo reduces the GPU memory footprint automatically and transparently without any changes required to the training source code, and is effective for models beyond LSTM RNNs. We evaluate Echo on numerous state-of-the-art machine learning workloads, including NMT, DeepSpeech2, Transformer, and ResNet, on real systems with modern GPUs and observe footprint reduction ratios of 1. 89x on average and 3. 13x maximum. Such reduction can be converted into faster training with a larger batch size, savings in GPU energy consumption (e.g., training with one GPU as fast as with four), and/or an increase in the maximum number of layers under the same GPU memory budget. Echo is open-sourced as a part of the MXNet 2.0 framework. 11https://issues.apache.org/jirdprojects/MXNET/issues/MXNET-1450 },
keywords = {},
doi = {10.1109/ISCA45697.2020.00092},
url = {https://doi.ieeecomputersociety.org/10.1109/ISCA45697.2020.00092},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@phdthesis{gradient-clipping,
    author = "Tom\'{a}\v{s} Mikolov",
    type = "Ph.D. thesis",
    title = "STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS",
    school = "Brno University of Technology, Faculty of Information Technology",
    year = 2012,
    location = "Brno, CZ",
    language = "english",
    url = "https://www.fit.vut.cz/study/phd-thesis/283/"
}

@InProceedings{cahuantzi2023lstmvsgru,
author="Cahuantzi, Roberto
and Chen, Xinye
and G{\"u}ttel, Stefan",
editor="Arai, Kohei",
title="A Comparison of LSTM and GRU Networks for Learning Symbolic Sequences",
booktitle="Intelligent Computing",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="771--785",
abstract="We explore the architecture of recurrent neural networks (RNNs) by studying the complexity of string sequences that it is able to memorize. Symbolic sequences of different complexity are generated to simulate RNN training and study parameter configurations with a view to the network's capability of learning and inference. We compare Long Short-Term Memory (LSTM) networks and gated recurrent units (GRUs). We find that an increase in RNN depth does not necessarily result in better memorization capability when the training time is constrained. Our results also indicate that the learning rate and the number of units per layer are among the most important hyper-parameters to be tuned. Generally, GRUs outperform LSTM networks on low-complexity sequences while on high-complexity sequences LSTMs perform better.",
isbn="978-3-031-37963-5"
}

@article{bolboaca2023lstmperformance,
author = {Bolboaca, Roland and Piroska, Haller},
year = {2023},
month = {03},
pages = {1432},
title = {Performance Analysis of Long Short-Term Memory Predictive Neural Networks on Time Series Data},
volume = {11},
journal = {Mathematics},
doi = {10.3390/math11061432}
}

@InProceedings{gers2001timeseries,
author="Gers, Felix A.
and Eck, Douglas
and Schmidhuber, J{\"u}rgen",
editor="Tagliaferri, Roberto
and Marinaro, Maria",
title="Applying LSTM to Time Series Predictable Through Time-Window Approaches",
booktitle="Neural Nets WIRN Vietri-01",
year="2002",
publisher="Springer London",
address="London",
pages="193--200",
abstract="Long Short-Term Memory (LSTM) is able to solve many time series tasks unsolvable by feed-forward networks using fixed size time windows. Here we find that LSTM's superiority does not carry over to certain simpler time series prediction tasks solvable by time window approaches: the Mackey-Glass series and the Santa Fe FIR laser emission series (Set A). This suggests to use LSTM only when simpler traditional approaches fail.",
isbn="978-1-4471-0219-9"
}

@InProceedings{eck2002musicgeneration,
  author={Eck, D. and Schmidhuber, J.},
  booktitle={Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing}, 
  title={Finding temporal structure in music: blues improvisation with LSTM recurrent networks}, 
  year={2002},
  volume={},
  number={},
  pages={747-756},
  keywords={Intelligent networks;Multiple signal classification;Recurrent neural networks;Timing;Adaptive signal processing;Signal generators;Signal processing;Machine learning;Bars;Learning systems},
  doi={10.1109/NNSP.2002.1030094}
}

@article{werb1990bptt,
  author={Werbos, P.J.},
  journal={Proceedings of the IEEE}, 
  title={Backpropagation through time: what it does and how to do it}, 
  year={1990},
  volume={78},
  number={10},
  pages={1550-1560},
  keywords={Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi={10.1109/5.58337}}

@article{torres2022elctricityforecasting,
    author = {Torres, J.F. and Martínez-Álvárez, F. and Troncoso, A.},
    title = {"A deep LSTM network for the Spanish electricity consumption forecasting"},
    journal = {"Neural Computing and Applications "},
    year = {2022},
    volume = {34},
    pages = {10533-10545}
}

@Article{nielsen2024electricitypriceforcasting,
AUTHOR = {Kılıç, Deniz Kenan and Nielsen, Peter and Thibbotuwawa, Amila},
TITLE = {Intraday Electricity Price Forecasting via LSTM and Trading Strategy for the Power Market: A Case Study of the West Denmark DK1 Grid Region},
JOURNAL = {Energies},
VOLUME = {17},
YEAR = {2024},
NUMBER = {12},
ARTICLE-NUMBER = {2909},
URL = {https://www.mdpi.com/1996-1073/17/12/2909},
ISSN = {1996-1073},
ABSTRACT = {For several stakeholders, including market players, customers, grid operators, policy-makers, investors, and energy efficiency initiatives, having a precise estimate of power pricing is crucial. It is easier for traders to plan, purchase, and sell power transactions with access to accurate electricity price forecasting (EPF). Although energy production and consumption topics are widely discussed in the literature, EPF and renewable energy trading studies receive less attention, especially for intraday market modeling and forecasting. Considering the rapid development of renewable energy sources, the article highlights the significance of integrating the deep learning model, long short-term memory (LSTM), with the proper trading strategy for short-term hourly renewable energy trading by utilizing two different spot markets. Day-ahead and intraday markets are taken into account for the West Denmark grid region (DK1). The time series analysis indicates that LSTM yields superior results compared to other benchmark machine learning algorithms. Using the predictions obtained by LSTM and the recommended trading strategy, promising profit values are achieved for the DK1 wind and solar energy use case, which ensures future motivation to develop a general and flexible model for global data.},
DOI = {10.3390/en17122909}
}

@inproceedings{chang2018adamlstm,
  author={Chang, Zihan and Zhang, Yang and Chen, Wenbo},
  booktitle={2018 IEEE 9th International Conference on Software Engineering and Service Science (ICSESS)}, 
  title={Effective Adam-Optimized LSTM Neural Network for Electricity Price Forecasting}, 
  year={2018},
  volume={},
  number={},
  pages={245-248},
  keywords={Forecasting;Predictive models;Recurrent neural networks;Stochastic processes;Logic gates;Optimization;component;Electricity price;Adam;Long short-term memory},
  doi={10.1109/ICSESS.2018.8663710}
}

@article{terven2025losses,
    author = {Terven, Juan and Cordova-Esparza, Diana-Margarita and Romero-González, Julio-Alejandro and Ramírez-Pedraza, Alfonso and Chávez-Urbiola, E. A.},
    title = {A comprehensive survey of loss functions and metrics in deep learning},
    journal = {Artificial Intelligence Review},
    year = {2025},
 pages = {1573--7462}
}

@article{willmott2005losses,
author = {Willmott, C. and Matsuura, K},
year = {2005},
month = {12},
pages = {79},
title = {Advantages of the Mean Absolute Error (MAE) over the Root Mean Square Error (RMSE) in Assessing Average Model Performance},
volume = {30},
journal = {Climate Research},
doi = {10.3354/cr030079}
}

@article{harris1954languagestructure,
author = {Zellig S. Harris},
title = {Distributional Structure},
journal = {WORD},
volume = {10},
number = {2-3},
pages = {146-162},
year = {1954},
publisher = {Routledge},
doi = {10.1080/00437956.1954.11659520},
URL = { https://doi.org/10.1080/00437956.1954.11659520 },
eprint = { https://doi.org/10.1080/00437956.1954.11659520}
}

@ARTICLE{gers2000lstmnlp,
  author={Gers, F.A. and Schmidhuber, E.},
  journal={IEEE Transactions on Neural Networks}, 
  title={LSTM recurrent networks learn simple context-free and context-sensitive languages}, 
  year={2001},
  volume={12},
  number={6},
  pages={1333-1340},
  keywords={Recurrent neural networks;Hidden Markov models;Delay effects;Backpropagation algorithms;Resonance light scattering;Learning automata;Neural networks;State-space methods;Bridges;Computational complexity},
  doi={10.1109/72.963769}
}

@article{nguyen2022lstmforgetgraphic,
author = {Nguyen, Anh and Nguyen, Phi Le and Vu, Viet and Pham, Quoc and Nguyen, Viet and Nguyen, Minh Hieu and Nguyen, Hùng and Nguyen, Kien},
year = {2022},
month = {11},
pages = {},
title = {Accurate discharge and water level forecasting using ensemble learning with genetic algorithm and singular spectrum analysis-based denoising},
volume = {12},
journal = {Scientific Reports},
doi = {10.1038/s41598-022-22057-8}
}

@INPROCEEDINGS{gers1999forgetgate,
  author={Gers, F.A. and Schmidhuber, J. and Cummins, F.},
  booktitle={1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)}, 
  title={Learning to forget: continual prediction with LSTM}, 
  year={1999},
  volume={2},
  number={},
  pages={850-855 vol.2},
  keywords={},
  doi={10.1049/cp:19991218}}


@ARTICLE{gokmen2018hadamard,
  
AUTHOR={Gokmen, Tayfun  and Rasch, Malte J.  and Haensch, Wilfried },
         
TITLE={Training LSTM Networks With Resistive Cross-Point Devices},
        
JOURNAL={Frontiers in Neuroscience},
        
VOLUME={Volume 12 - 2018},

YEAR={2018},

URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2018.00745},

DOI={10.3389/fnins.2018.00745},

ISSN={1662-453X},

ABSTRACT={In our previous work we have shown that resistive cross point devices, so called Resistive Processing Unit (RPU) devices, can provide significant power and speed benefits when training deep fully connected networks as well as convolutional neural networks. In this work, we further extend the RPU concept for training recurrent neural networks (RNNs) namely LSTMs. We show that the mapping of recurrent layers is very similar to the mapping of fully connected layers and therefore the RPU concept can potentially provide large acceleration factors for RNNs as well. In addition, we study the effect of various device imperfections and system parameters on training performance. Symmetry of updates becomes even more crucial for RNNs; already a few percent asymmetry results in an increase in the test error compared to the ideal case trained with floating point numbers. Furthermore, the input signal resolution to the device arrays needs to be at least 7 bits for successful training. However, we show that a stochastic rounding scheme can reduce the input signal resolution back to 5 bits. Further, we find that RPU device variations and hardware noise are enough to mitigate overfitting, so that there is less need for using dropout. Here we attempt to study the validity of the RPU approach by simulating large scale networks. For instance, the models studied here are roughly 1500 times larger than the more often studied multilayer perceptron models trained on the MNIST dataset in terms of the total number of multiplication and summation operations performed per epoch. }}

@misc{sak2014longshorttermmemorybased,
      title={Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}, 
      author={Haşim Sak and Andrew Senior and Françoise Beaufays},
      year={2014},
      eprint={1402.1128},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1402.1128}, 
}


@article{Rosenblatt58,
  author  = {Frank Rosenblatt},
  title   = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
  journal = {Psychological Review},
  year    = {1958},
  volume  = {65},
  number  = {6},
  pages   = {386--408},
  doi     = {10.1037/h0042519}
}

@book{MinskyPapert69,
  author    = {Marvin Minsky and Seymour Papert},
  title     = {Perceptrons: An Introduction to Computational Geometry},
  publisher = {MIT Press},
  address   = {Cambridge, MA},
  year      = {1969}
}

@article{Rumelhart86,
  author  = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title   = {Learning Representations by Back-Propagating Errors},
  journal = {Nature},
  year    = {1986},
  volume  = {323},
  pages   = {533--536},
  doi     = {10.1038/323533a0}
}

@article{Elman90,
  author  = {Jeffrey L. Elman},
  title   = {Finding Structure in Time},
  journal = {Cognitive Science},
  year    = {1990},
  volume  = {14},
  number  = {2},
  pages   = {179--211},
  doi     = {10.1207/s15516709cog1402_1}
}

@incollection{Bottou10,
  author    = {Léon Bottou},
  title     = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  booktitle = {Proceedings of COMPSTAT'2010},
  editor    = {Yves Lechevallier and Gilbert Saporta},
  publisher = {Physica-Verlag HD},
  pages     = {177--186},
  year      = {2010},
  doi       = {10.1007/978-3-7908-2604-3_16}
}

@article{alselwi2024lstmfuture,
title = {RNN-LSTM: From applications to modeling techniques and beyond—Systematic review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {5},
pages = {102068},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102068},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824001575},
author = {Safwan Mahmood Al-Selwi and Mohd Fadzil Hassan and Said Jadid Abdulkadir and Amgad Muneer and Ebrahim Hamid Sumiea and Alawi Alqushaibi and Mohammed Gamal Ragab},
keywords = {Machine learning, Deep learning, Recurrent neural networks, Long short-term memory, Weights initialization, Weights optimization, Systematic literature review},
abstract = {Long Short-Term Memory (LSTM) is a popular Recurrent Neural Network (RNN) algorithm known for its ability to effectively analyze and process sequential data with long-term dependencies. Despite its popularity, the challenge of effectively initializing and optimizing RNN-LSTM models persists, often hindering their performance and accuracy. This study presents a systematic literature review (SLR) using an in-depth four-step approach based on the PRISMA methodology, incorporating peer-reviewed articles spanning 2018–2023. It aims to address how weight initialization and optimization techniques can bolster RNN-LSTM performance. This SLR offers a detailed overview across various applications and domains, and stands out by comprehensively analyzing modeling techniques, datasets, evaluation metrics, and programming languages associated with these networks. The findings of this SLR provide a roadmap for researchers and practitioners to enhance RNN-LSTM networks and achieve superior results.}
}
@Article{zhao2025lstmtransformerhybrid,
AUTHOR = {Zhao, Yali and Guo, Yingying and Wang, Xuecheng},
TITLE = {Hybrid LSTM–Transformer Architecture with Multi-Scale Feature Fusion for High-Accuracy Gold Futures Price Forecasting},
JOURNAL = {Mathematics},
VOLUME = {13},
YEAR = {2025},
NUMBER = {10},
ARTICLE-NUMBER = {1551},
URL = {https://www.mdpi.com/2227-7390/13/10/1551},
ISSN = {2227-7390},
ABSTRACT = {Amidst global economic fluctuations and escalating geopolitical risks, gold futures, as a pivotal safe-haven asset, demonstrate price dynamics that directly impact investor decision-making and risk mitigation effectiveness. Traditional forecasting models face significant limitations in capturing long-term trends, addressing abrupt volatility, and mitigating multi-source noise within complex market environments characterized by nonlinear interactions and extreme events. Current research predominantly focuses on single-model approaches (e.g., ARIMA or standalone neural networks), inadequately addressing the synergistic effects of multimodal market signals (e.g., cross-market index linkages, exchange rate fluctuations, and policy shifts) and lacking the systematic validation of model robustness under extreme events. Furthermore, feature selection often relies on empirical assumptions, failing to uncover non-explicit correlations between market factors and gold futures prices. A review of the global literature reveals three critical gaps: (1) the insufficient integration of temporal dependency and global attention mechanisms, leading to imbalanced predictions of long-term trends and short-term volatility; (2) the neglect of dynamic coupling effects among cross-market risk factors, such as energy ETF-metal market spillovers; and (3) the absence of hybrid architectures tailored for high-frequency noise environments, limiting predictive utility for decision support. This study proposes a three-stage LSTM–Transformer–XGBoost fusion framework. Firstly, XGBoost-based feature importance ranking identifies six key drivers from thirty-six candidate indicators: the NASDAQ Index, S&P 500 closing price, silver futures, USD/CNY exchange rate, China’s 1-year Treasury yield, and Guotai Zhongzheng Coal ETF. Second, a dual-channel deep learning architecture integrates LSTM for long-term temporal memory and Transformer with multi-head self-attention to decode implicit relationships in unstructured signals (e.g., market sentiment and climate policies). Third, rolling-window forecasting is conducted using daily gold futures prices from the Shanghai Futures Exchange (2015–2025). Key innovations include the following: (1) a bidirectional LSTM–Transformer interaction architecture employing cross-attention mechanisms to dynamically couple global market context with local temporal features, surpassing traditional linear combinations; (2) a Dynamic Hierarchical Partition Framework (DHPF) that stratifies data into four dimensions (price trends, volatility, external correlations, and event shocks) to address multi-driver complexity; (3) a dual-loop adaptive mechanism enabling endogenous parameter updates and exogenous environmental perception to minimize prediction error volatility. This research proposes innovative cross-modal fusion frameworks for gold futures forecasting, providing financial institutions with robust quantitative tools to enhance asset allocation optimization and strengthen risk hedging strategies. It also provides an interpretable hybrid framework for derivative pricing intelligence. Future applications could leverage high-frequency data sharing and cross-market risk contagion models to enhance China’s influence in global gold pricing governance.},
DOI = {10.3390/math13101551}
}
