@article{hochreiter1997lstm,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  year={1997},
  volume={9},
  number={8},
  pages={1735–1780}
}

@InProceedings{zheng2018scalability,
  author={Zhu, Hongyu and Akrout, Mohamed and Zheng, Bojian and Pelegris, Andrew and Jayarajan, Anand and Phanishayee, Amar and Schroeder, Bianca and Pekhimenko, Gennady},
  booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)}, 
  title={Benchmarking and Analyzing Deep Neural Network Training}, 
  year={2018},
  volume={},
  number={},
  pages={88-100},
  keywords={Training;Benchmark testing;Tools;Graphics processing units;Computational modeling;Speech recognition;Hardware},
  doi={10.1109/IISWC.2018.8573476}}

@InProceedings {zheng2020scalability,
author = { Zheng, Bojian and Vijaykumar, Nandita and Pekhimenko, Gennady },
booktitle = { 2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA) },
title = {{ Echo: Compiler-based GPU Memory Footprint Reduction for LSTM RNN Training }},
year = {2020},
volume = {},
ISSN = {},
pages = {1089-1102},
abstract = { The Long-Short-Term-Memory Recurrent Neural Networks (LSTM RNNs) are a popular class of machine learning models for analyzing sequential data. Their training on modern GPUs, however, is limited by the GPU memory capacity. Our profiling results of the LSTM RNN-based Neural Machine Translation (NMT) model reveal that feature maps of the attention and RNN layers form the memory bottleneck, and runtime is unevenly distributed across different layers when training on GPUs. Based on these two observations, we propose to recompute the feature maps of the attention and RNN layers rather than stashing them persistently in the GPU memory. While the idea of feature map recomputation has been considered before, existing solutions fail to deliver satisfactory footprint reduction, as they do not address two key challenges. For each feature map recomputation to be efficient, its effect on (1) the total memory footprint, and (2) the total execution time has to be carefully estimated. To this end, we propose Echo, a new compiler-based optimization scheme that addresses the first challenge with a practical mechanism that estimates the memory benefits of recomputation over the entire computation graph, and the second challenge by non-conservatively estimating the recomputation runtime overhead leveraging layer specifics. Echo reduces the GPU memory footprint automatically and transparently without any changes required to the training source code, and is effective for models beyond LSTM RNNs. We evaluate Echo on numerous state-of-the-art machine learning workloads, including NMT, DeepSpeech2, Transformer, and ResNet, on real systems with modern GPUs and observe footprint reduction ratios of 1. 89x on average and 3. 13x maximum. Such reduction can be converted into faster training with a larger batch size, savings in GPU energy consumption (e.g., training with one GPU as fast as with four), and/or an increase in the maximum number of layers under the same GPU memory budget. Echo is open-sourced as a part of the MXNet 2.0 framework. 11https://issues.apache.org/jirdprojects/MXNET/issues/MXNET-1450 },
keywords = {},
doi = {10.1109/ISCA45697.2020.00092},
url = {https://doi.ieeecomputersociety.org/10.1109/ISCA45697.2020.00092},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month =Jun}

@phdthesis{gradient-clipping,
    author = "Tom\'{a}\v{s} Mikolov",
    type = "Ph.D. thesis",
    title = "STATISTICAL LANGUAGE MODELS BASED ON NEURAL NETWORKS",
    school = "Brno University of Technology, Faculty of Information Technology",
    year = 2012,
    location = "Brno, CZ",
    language = "english",
    url = "https://www.fit.vut.cz/study/phd-thesis/283/"
}

@article{bolboaca2023lstmperformance,
author = {Bolboaca, Roland and Piroska, Haller},
year = {2023},
month = {03},
pages = {1432},
title = {Performance Analysis of Long Short-Term Memory Predictive Neural Networks on Time Series Data},
volume = {11},
journal = {Mathematics},
doi = {10.3390/math11061432}
}

@InProceedings{gers2001timeseries,
author="Gers, Felix A.
and Eck, Douglas
and Schmidhuber, J{\"u}rgen",
editor="Tagliaferri, Roberto
and Marinaro, Maria",
title="Applying LSTM to Time Series Predictable Through Time-Window Approaches",
booktitle="Neural Nets WIRN Vietri-01",
year="2002",
publisher="Springer London",
address="London",
pages="193--200",
abstract="Long Short-Term Memory (LSTM) is able to solve many time series tasks unsolvable by feed-forward networks using fixed size time windows. Here we find that LSTM's superiority does not carry over to certain simpler time series prediction tasks solvable by time window approaches: the Mackey-Glass series and the Santa Fe FIR laser emission series (Set A). This suggests to use LSTM only when simpler traditional approaches fail.",
isbn="978-1-4471-0219-9"
}

@InProceedings{eck2002musicgeneration,
  author={Eck, D. and Schmidhuber, J.},
  booktitle={Proceedings of the 12th IEEE Workshop on Neural Networks for Signal Processing}, 
  title={Finding temporal structure in music: blues improvisation with LSTM recurrent networks}, 
  year={2002},
  volume={},
  number={},
  pages={747-756},
  keywords={Intelligent networks;Multiple signal classification;Recurrent neural networks;Timing;Adaptive signal processing;Signal generators;Signal processing;Machine learning;Bars;Learning systems},
  doi={10.1109/NNSP.2002.1030094}
}

@article{torres2022elctricityforecasting,
    author = {Torres, J.F. and Martínez-Álvárez, F. and Troncoso, A.},
    title = {"A deep LSTM network for the Spanish electricity consumption forecasting"},
    journal = {"Neural Computing and Applications "},
    year = {2022},
    volume = {34},
    pages = {10533-10545}
}

@Article{nielsen2024electricitypriceforcasting,
AUTHOR = {Kılıç, Deniz Kenan and Nielsen, Peter and Thibbotuwawa, Amila},
TITLE = {Intraday Electricity Price Forecasting via LSTM and Trading Strategy for the Power Market: A Case Study of the West Denmark DK1 Grid Region},
JOURNAL = {Energies},
VOLUME = {17},
YEAR = {2024},
NUMBER = {12},
ARTICLE-NUMBER = {2909},
URL = {https://www.mdpi.com/1996-1073/17/12/2909},
ISSN = {1996-1073},
ABSTRACT = {For several stakeholders, including market players, customers, grid operators, policy-makers, investors, and energy efficiency initiatives, having a precise estimate of power pricing is crucial. It is easier for traders to plan, purchase, and sell power transactions with access to accurate electricity price forecasting (EPF). Although energy production and consumption topics are widely discussed in the literature, EPF and renewable energy trading studies receive less attention, especially for intraday market modeling and forecasting. Considering the rapid development of renewable energy sources, the article highlights the significance of integrating the deep learning model, long short-term memory (LSTM), with the proper trading strategy for short-term hourly renewable energy trading by utilizing two different spot markets. Day-ahead and intraday markets are taken into account for the West Denmark grid region (DK1). The time series analysis indicates that LSTM yields superior results compared to other benchmark machine learning algorithms. Using the predictions obtained by LSTM and the recommended trading strategy, promising profit values are achieved for the DK1 wind and solar energy use case, which ensures future motivation to develop a general and flexible model for global data.},
DOI = {10.3390/en17122909}
}

@article{harris1954languagestructure,
author = {Zellig S. Harris},
title = {Distributional Structure},
journal = {WORD},
volume = {10},
number = {2-3},
pages = {146-162},
year = {1954},
publisher = {Routledge},
doi = {10.1080/00437956.1954.11659520},
URL = { https://doi.org/10.1080/00437956.1954.11659520 },
eprint = { https://doi.org/10.1080/00437956.1954.11659520}
}

@article{nguyen2022lstmforgetgraphic,
author = {Nguyen, Anh and Nguyen, Phi Le and Vu, Viet and Pham, Quoc and Nguyen, Viet and Nguyen, Minh Hieu and Nguyen, Hùng and Nguyen, Kien},
year = {2022},
month = {11},
pages = {},
title = {Accurate discharge and water level forecasting using ensemble learning with genetic algorithm and singular spectrum analysis-based denoising},
volume = {12},
journal = {Scientific Reports},
doi = {10.1038/s41598-022-22057-8}
}

@INPROCEEDINGS{gers1999forgetgate,
  author={Gers, F.A. and Schmidhuber, J. and Cummins, F.},
  booktitle={1999 Ninth International Conference on Artificial Neural Networks ICANN 99. (Conf. Publ. No. 470)}, 
  title={Learning to forget: continual prediction with LSTM}, 
  year={1999},
  volume={2},
  number={},
  pages={850-855 vol.2},
  keywords={},
  doi={10.1049/cp:19991218}}

@ARTICLE{gokmen2018hadamard,
  
AUTHOR={Gokmen, Tayfun  and Rasch, Malte J.  and Haensch, Wilfried },
         
TITLE={Training LSTM Networks With Resistive Cross-Point Devices},
        
JOURNAL={Frontiers in Neuroscience},
        
VOLUME={Volume 12 - 2018},

YEAR={2018},

URL={https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2018.00745},

DOI={10.3389/fnins.2018.00745},

ISSN={1662-453X},

ABSTRACT={In our previous work we have shown that resistive cross point devices, so called Resistive Processing Unit (RPU) devices, can provide significant power and speed benefits when training deep fully connected networks as well as convolutional neural networks. In this work, we further extend the RPU concept for training recurrent neural networks (RNNs) namely LSTMs. We show that the mapping of recurrent layers is very similar to the mapping of fully connected layers and therefore the RPU concept can potentially provide large acceleration factors for RNNs as well. In addition, we study the effect of various device imperfections and system parameters on training performance. Symmetry of updates becomes even more crucial for RNNs; already a few percent asymmetry results in an increase in the test error compared to the ideal case trained with floating point numbers. Furthermore, the input signal resolution to the device arrays needs to be at least 7 bits for successful training. However, we show that a stochastic rounding scheme can reduce the input signal resolution back to 5 bits. Further, we find that RPU device variations and hardware noise are enough to mitigate overfitting, so that there is less need for using dropout. Here we attempt to study the validity of the RPU approach by simulating large scale networks. For instance, the models studied here are roughly 1500 times larger than the more often studied multilayer perceptron models trained on the MNIST dataset in terms of the total number of multiplication and summation operations performed per epoch. }}

@Article{zhao2025lstmtransformerhybrid,
AUTHOR = {Zhao, Yali and Guo, Yingying and Wang, Xuecheng},
TITLE = {Hybrid LSTM–Transformer Architecture with Multi-Scale Feature Fusion for High-Accuracy Gold Futures Price Forecasting},
JOURNAL = {Mathematics},
VOLUME = {13},
YEAR = {2025},
NUMBER = {10},
ARTICLE-NUMBER = {1551},
URL = {https://www.mdpi.com/2227-7390/13/10/1551},
ISSN = {2227-7390},
ABSTRACT = {Amidst global economic fluctuations and escalating geopolitical risks, gold futures, as a pivotal safe-haven asset, demonstrate price dynamics that directly impact investor decision-making and risk mitigation effectiveness. Traditional forecasting models face significant limitations in capturing long-term trends, addressing abrupt volatility, and mitigating multi-source noise within complex market environments characterized by nonlinear interactions and extreme events. Current research predominantly focuses on single-model approaches (e.g., ARIMA or standalone neural networks), inadequately addressing the synergistic effects of multimodal market signals (e.g., cross-market index linkages, exchange rate fluctuations, and policy shifts) and lacking the systematic validation of model robustness under extreme events. Furthermore, feature selection often relies on empirical assumptions, failing to uncover non-explicit correlations between market factors and gold futures prices. A review of the global literature reveals three critical gaps: (1) the insufficient integration of temporal dependency and global attention mechanisms, leading to imbalanced predictions of long-term trends and short-term volatility; (2) the neglect of dynamic coupling effects among cross-market risk factors, such as energy ETF-metal market spillovers; and (3) the absence of hybrid architectures tailored for high-frequency noise environments, limiting predictive utility for decision support. This study proposes a three-stage LSTM–Transformer–XGBoost fusion framework. Firstly, XGBoost-based feature importance ranking identifies six key drivers from thirty-six candidate indicators: the NASDAQ Index, S&P 500 closing price, silver futures, USD/CNY exchange rate, China’s 1-year Treasury yield, and Guotai Zhongzheng Coal ETF. Second, a dual-channel deep learning architecture integrates LSTM for long-term temporal memory and Transformer with multi-head self-attention to decode implicit relationships in unstructured signals (e.g., market sentiment and climate policies). Third, rolling-window forecasting is conducted using daily gold futures prices from the Shanghai Futures Exchange (2015–2025). Key innovations include the following: (1) a bidirectional LSTM–Transformer interaction architecture employing cross-attention mechanisms to dynamically couple global market context with local temporal features, surpassing traditional linear combinations; (2) a Dynamic Hierarchical Partition Framework (DHPF) that stratifies data into four dimensions (price trends, volatility, external correlations, and event shocks) to address multi-driver complexity; (3) a dual-loop adaptive mechanism enabling endogenous parameter updates and exogenous environmental perception to minimize prediction error volatility. This research proposes innovative cross-modal fusion frameworks for gold futures forecasting, providing financial institutions with robust quantitative tools to enhance asset allocation optimization and strengthen risk hedging strategies. It also provides an interpretable hybrid framework for derivative pricing intelligence. Future applications could leverage high-frequency data sharing and cross-market risk contagion models to enhance China’s influence in global gold pricing governance.},
DOI = {10.3390/math13101551}
}

@article{alselwi2024lstmfuture,
title = {RNN-LSTM: From applications to modeling techniques and beyond—Systematic review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {5},
pages = {102068},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102068},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824001575},
author = {Safwan Mahmood Al-Selwi and Mohd Fadzil Hassan and Said Jadid Abdulkadir and Amgad Muneer and Ebrahim Hamid Sumiea and Alawi Alqushaibi and Mohammed Gamal Ragab},
keywords = {Machine learning, Deep learning, Recurrent neural networks, Long short-term memory, Weights initialization, Weights optimization, Systematic literature review},
abstract = {Long Short-Term Memory (LSTM) is a popular Recurrent Neural Network (RNN) algorithm known for its ability to effectively analyze and process sequential data with long-term dependencies. Despite its popularity, the challenge of effectively initializing and optimizing RNN-LSTM models persists, often hindering their performance and accuracy. This study presents a systematic literature review (SLR) using an in-depth four-step approach based on the PRISMA methodology, incorporating peer-reviewed articles spanning 2018–2023. It aims to address how weight initialization and optimization techniques can bolster RNN-LSTM performance. This SLR offers a detailed overview across various applications and domains, and stands out by comprehensively analyzing modeling techniques, datasets, evaluation metrics, and programming languages associated with these networks. The findings of this SLR provide a roadmap for researchers and practitioners to enhance RNN-LSTM networks and achieve superior results.}
}

@misc{hochreiter1991,
  author = {J. Hochreiter},
  title = {Untersuchungen zu dynamischen neuronalen Netzen},
  howpublished = {https://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf},
  year = {1991},
  note = {Accessed: June 2025},
}

@article{elman90,
  author  = {Jeffrey L. Elman},
  title   = {Finding Structure in Time},
  journal = {Cognitive Science},
  year    = {1990},
  volume  = {14},
  number  = {2},
  pages   = {179--211},
  doi     = {10.1207/s15516709cog1402_1}
}

@article{werb1990bptt,
  author={Werbos, P.J.},
  journal={Proceedings of the IEEE},
  title={Backpropagation through time: what it does and how to do it},
  year={1990},
  volume={78},
  number={10},
  pages={1550-1560},
  keywords={Backpropagation;Artificial neural networks;Supervised learning;Pattern recognition;Neural networks;Power system modeling;Equations;Control systems;Fluid dynamics;Books},
  doi={10.1109/5.58337}
}

@InProceedings{pascanu2013rnntraining,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/pascanu13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/pascanu13.html},
  abstract = 	 {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.  }
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}
