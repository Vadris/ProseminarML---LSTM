% Template for seminar reports
% Computer Vision Group, Visual Computing Institute, RWTH Aachen University
\documentclass[twoside,a4paper,10pt,DIV=12,BCOR=12mm]{scrartcl}
\usepackage[utf8]{inputenc}    % allows special symbols in LaTeX source code
\usepackage[english]{babel}    % select language of your report (use "ngerman" for German)
\usepackage[T1]{fontenc}       % font encoding, important for umlauts
\usepackage{lmodern}           % lmodern font
\usepackage{xcolor}            % use and define colors for text and images
\usepackage{graphicx}          % include images
\usepackage{booktabs}          % pretty tables
\usepackage{caption}           % captions for figures
\usepackage{subcaption}        % captions for subfigures
\usepackage{url}               % for website links
\usepackage{datetime}          % month on title page
\usepackage{hyperref}          % make references clickable
\usepackage{xspace}            % correct spaces after e.g. "e.g."
\usepackage{tikz}              % custom plots in LaTeX
\usepackage{pgfplots}          % custom function graphs in LaTeX
\usepackage{bm}                % bold math symbols
\usepackage{amsmath}           % additional math environments
\usepackage{amssymb}           % additional math symbols (includes amsfonts)
\usepackage{enumitem}          % change distances for enumerations
\usepackage{listings}          % code
\usepackage{lipsum}            % placeholder text
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

\newdateformat{monthyeardate}{\monthname[\THEMONTH] \THEYEAR}

\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}
\setlist{nosep}

\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\cf{{c.f}\onedot} \def\Cf{\emph{C.f}\onedot}
\def\etc{{etc}\onedot} \def\vs{\emph{vs}\onedot}
\def\wrt{w.r.t\onedot} \def\dof{d.o.f\onedot}
\def\etal{\emph{et al}\onedot}
\makeatother

\overfullrule=1ex

\pgfplotsset{compat=newest}


\tikzset{basic/.style={draw,fill=none,
                       text badly centered,minimum width=3em}}
\tikzset{input/.style={basic,circle,minimum width=3.5em}}
\tikzset{weights/.style={basic,rectangle,minimum width=2em}}
\tikzset{functions/.style={basic,circle, minimum width=3em}}
\newcommand{\addaxes}{\draw (0em,1em) -- (0em,-1em)
                            (-1em,0em) -- (1em,0em);}
\newcommand{\relu}{\draw[line width=1.5pt] (-1em,0) -- (0,0)
                                (0,0) -- (0.75em,0.75em);}
\newcommand{\stepfunc}{\draw[line width=1.5pt] (0.65em,0.65em) -- (0,0.65em) 
                                    -- (0,-0.65em) -- (-0.65em,-0.65em);}


% =========================================================================

\graphicspath{{pictures/}}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}

% =========================================================================
\begin{document}

\include{titlepage}

\begin{abstract}
\textbf{Abstract (Fynn).} Long Short-Term Memory (LSTM) is a widely used type of recurrent neural network designed to address the vanishing gradient problem. This paper provides an overview of the foundational concepts that led to its development, followed by an explanation of the architecture A PyTorch implementation is showcased and tested on a simple sequence. Limitations of the original design are discussed, that lead to variations like the commonly added forget gate. The architecture is compared with alternative architectures, such as Transformers and applications of the architecture in areas such as natural language processing (NLP) and time series forecasting are highlighted.
\end{abstract}

\tableofcontents
\newpage
% =========================================================================

\section{Introduction (Fynn)}

Much of the data we encounter in the real world, such as natural language, speech, music and weather, is best described as a sequence of individual data points that must be interpreted in context to extract meaningful information, such as patterns.\\
For instance, individual words do not carry much information and can have multiple meanings depending on the context; however, if multiple words are arranged in a specific order, they can convey complex ideas. Additionally, the order of the words cannot be completely random; it must follow a specific structure to form a proper sentence. This means that, for tasks such as translation, it is important to take the entire context into account in order to properly interpret the meaning of individual words.\cite{harris1954languagestructure}\\
Similarly, most forms of music contain structures and patterns, such as rhythms, chord progressions or melodic motives, which convey meaning; individual notes or transients, on the other hand, convey very little information on their own. Therefore, when writing chord progressions, for example, it is important to consider all previous chords when selecting a chord, as the context will change its sound and meaning.\cite{eck2002musicgeneration}\\
In theory Recurrent Neural Networks can pick up on those patterns, but the algorithms used to train these networks, namely "Back-Propagation Through Time" (BPTT) or "Real-Time Recurrent Learning" (RTRL) had the problem of vanishing or exploding gradients, which made training RNNs impractical, when compared to traditional Feedforward Neural Networks. This problem gets amplified as the minimum lag between time steps increases.\cite{hochreiter1997lstm,werb1990bptt}\\
The LSTM Architecture, as first described by Sepp Hochreiter and Jürgen Schmidhuber in their 1997 paper, "Long Short-Term Memory" and later refined by Felix A. Gers, Jürgen Schmidhuber and Fred Cummins in their paper "Learning to Forget: Continual Prediction with LSTM" published in 2000, aims to solve some of those issues, by introducing a modified version of the classical RNN structure, to improve the flow of the error signal through the network.\cite{hochreiter1997lstm}\\
Since then, LSTM networks have played a significant role in the development of machine learning-based sequence modeling, most notably in natural language processing, time series forecasting, music generation, and speech recognition.\cite{eck2002musicgeneration,torres2022elctricityforecasting,gers2001timeseries,nielsen2024electricitypriceforcasting, gers2000lstmnlp}\\
Section 2 will review a few important related works, then Section 3 will cover foundational concepts, such as feedforward neural networks and recurrent neural networks and will examine the problems that lead to the development of the LSTM architecture. Section 4 covers the principle of operation as well as mathematical formalization and experimental results.  In Section 5 limitations of the architecture will be examined as well as variations and alternatives, that might resolve some of those problems, and Section 6 examines possible applications for LSTMs.\\

\section{Related Work (Leon)}

This section will outline related papers about recurrent networks that were mentioned in \cite{hochreiter1997lstm}.


\paragraph{Multilayer perceptron.} The single‐layer perceptron \cite{Rosenblatt58} laid the conceptual 
foundation for neural computation, but it was limited to linearly separable problems. Adding hidden layers yields the
Multilayer Perceptron (MLP), which introduced distributed internal representations and universal function approximation capabilities \cite{Hornik89}.

\paragraph{Stochastic gradient descent.} Training neural nets at scale relies on stochastic gradient descent (SGD), in which 
parameter updates are computed from small, randomly sampled batches of data rather than the full dataset, dramatically reducing computation and memory costs \cite{Bottou10}.

\paragraph{Backpropagation.}
 The learning algorithm that made deep networks practical is backpropagation, first popularized for 
 feed‑forward nets \cite{Rumelhart86}.  For recurrent networks, 
 \cite{werb1990bptt} introduced \emph{backpropagation‑through‑time} (BPTT), unrolling the 
 network over the sequence length so that gradients can be accumulated across time steps.

\paragraph{Early recurrent networks.} The first recurrent networks namely Elman's Simple Recurrent Network (SRN) \cite{Elman90},
popularized the idea of hidden recurrent units, which develops internal representations of prior seen context.
They override their hidden state every iteration, which lead to difficulties in learning dependencies spanning more then
a couple time steps.

\paragraph{Vanishing gradient problem.} The vanishing / exploding gradient problem was already demonstrated by by Hochreiter \cite{hochreiter1991}.
He proved that products of Jacobian matrices let the gradients go towards zero or infinity. This in turn meant that the solution to
this issue is not a mere optimization of the RNN but requires architectural change.

\paragraph{Early attempts at long-term context.} Several ideas tried to save the problems of long term context retention.

\begin{itemize}
  \item{History Compressor.} A stack of autoencoders that should only update the hidden state when something 
    "suprising" happens, as in new information that should be remembered. This way the error flows properly during 
    intervals where nothing happens as in no new information is remembered \cite{Schmidhuber92}.
  \item{Continuous history compression.} Builds on top of the previous history compressor by extending it to 
    multiple time scales \cite{Schmidhuber93}.
\end{itemize}


\section{Foundational Concepts (Leon)}

Long Short-Term Memory models are a variant of Recurrent Neural Networks which is a type of Neural Network.
Neural Networks are the basis upon those architectures are build. This section aims to
provide a complete but rather shallow introduction into the foundations of neural networks.

\subsection{Neural Networks and Training}

\begin{center}
\begin{tikzpicture}
    % Draw input nodes
    \foreach \h [count=\hi ] in {$x_1$,$x_0$,$1$}{%
          \node[input] (f\hi) at (0,\hi*1.25cm-1.5 cm) {\h};
        }
    % Dot dot dot ... x_n
    \node[below=0.62cm] (idots) at (f1) {\vdots};
    \node[input, below=0.62cm] (last_input) at (idots) {$x_n$};
    % Draw summation node
    \node[functions] (sum) at (4,0) {\Huge$\sum$};
    \node[below=0.69cm] at (sum) {$\sum_{i=0}^n w_ix_i + b$};
    % Draw edges from input nodes to summation node
    \foreach \h [count=\hi ] in {$w_1$,$w_0$,$b$}{%
          \path (f\hi) -- node[weights] (w\hi) {\h} (sum);
          \draw[->] (f\hi) -- (w\hi);
          \draw[->] (w\hi) -- (sum);
        }
    % Dot dot dot ... w_n
    \node[below=0.05cm] (wdots) at (w1) {\vdots};
    \node[weights, below=0.45cm] (last_weight) at (wdots) {$w_n$};
    % Add edges for last node and last weight etc
    \path[draw,->] (last_input) -- (last_weight);
    \path[draw,->] (last_weight) -- (sum);
    % Draw node for activation function
    \node[functions] (activation) at (7,0) {$f$};
    % Place activation function in its node
    % \begin{scope}[xshift=7cm]
    %     \addaxes
    %     % flexible selection of activation function
    %     \relu
    %     % \stepfunc
    % \end{scope}
    % Connect sum to relu
    \draw[->] (sum) -- (activation);
    \draw[->] (activation) -- ++(1,0);
    % Labels
    \node[above=1cm]  at (f3) {inputs};
    \node[above=1cm] at (w3) {weights};
    \node[above=1cm] at (activation) {activation function};
\end{tikzpicture}

\captionof{figure}{A single Perceptron}
\label{fig:perceptron}

\end{center}
Neural networks are mostly build upon the Perceptron which can be thought of as the digital equivalent of a biological Neuron.
On a high level the Perceptron has a some number of numerical inputs and computes a singular output value based on that \cite{Rosenblatt58}.

A Perceptron is defined by its weights $w_i$, bias $b$ and the choice of the activation function $f$.
The bias is required so the Perceptron can output $y \neq 0$ when all $x_i = 0$.
Now having $n$ input values $x_0, x_1, ..., x_{n-1}, x_n$ the output of the perceptron can by computed like this:

$$
y=f\bigl(\sum_{i=0}^{n}w_i x_i + b\bigr)
$$


The figure \ref{fig:perceptron} shows such a perceptron.
A single basic Perceptron on its own is not particularly useful as
it only models linear decision boundaries and cannot learn the xor function \cite{MinskyPapert69}.

First we stack multiple Perceptrons to construct a layer. A layer has multiple
Perceptrons which calculate their output value with their own weights and biases on the same input values, which results
in as many output values as there are Perceptrons in the layer. Now one can construct a network which chains
multiple layers after each other, which results in a multilayer Perceptron (MLP), which is also known as a fully connected
feed forward network, as each layer is fully connected to the next layer in that each perceptron in layer $i$ 
is connected to each perceptron in layer $i+1$. The information strictly flows from input layer, through
intermediate hidden layers to the output layer, without any loops.
Mathematically a feed forward neural network, with $L$ layers and $0 < l \leq L$ can be described by

$$\mathbf{y}^{(l)} = f^{(l)}(\mathbf{W}^{(l)}\mathbf{y}^{(l-1)}+\mathbf{b}^{(l)})$$

where $\mathbf{W}^{(l)}$ represents the weight matrix of layer $l$, $\mathbf{b}^{(l)}$ the bias vector of layer $l$,
$\mathbf{y}^{(l-1)}$ the output vector of the previous layer or in case of $l=0$ the input vector
and $f^{(l)}$ is the activation function applied element-wise. \\


Now the choice of the activation function is very important, when using a multilayer Perceptron as when a linear activation function like the 
identity function is used for every Percetpron the entire network can be described in a single layer. Therefore
a non-linear activation function should be used to make the network capable of learning non-linear
decision boundaries. When choosing an activation function there are some important
properties which should be respected while making such a choice. The activation function should be fully differentiable, which makes
computing the gradient easier. It should also be computationally
cheap as its being run many times during a forward pass. \\

With loss functions the performance of a neural network on a given input can be computed by comparing the desired output with the 
predicted output of the model. Using loss functions we can quantify the error in the prediction of the neural network, which 
is a metric we can optimize for. There are again many options for choosing a loss function but it heavily depends on the nature of the
problem you're trying to solve by using a model: either classification or regression. A common loss function to use when trying to optimize 
regression tasks is the Mean Squared Error (MSE) \cite{Goodfellow16}:

$$\mathcal{L}(\mathbf{y},\mathbf{\hat{y}}) = \frac{1}{n}\sum^n_{i=1}{(y_i-\hat{y}_i)^2}$$

where $\mathbf{y}$ is the prediction of the model and $\mathbf{\hat{y}}$ the ground truth. When solving classification tasks the Cross Entropy Loss function is often used \cite{Goodfellow16}:


$$\mathcal{L}(\mathbf{y},\mathbf{\hat{y}}) = -\sum^n_{i=1}{y_i\log(\hat{y}_i)}$$

With loss functions we have a metric to optimize against, which can be used in an optimization algorithm called 
gradient descent. It will minimize the loss function by iteratively computing the error gradient and updating
the weights in the opposite direction of the gradient of the loss function. This is the update rule according
to gradient descent: \cite{Bottou10, Goodfellow16}

$$ \theta \leftarrow \theta - \eta \nabla_{\theta}\mathcal{L}(\theta)$$

Here, $\theta$ represents the parameters, $\eta$ the learning rate that determines the size of the step we take in 
the opposite direction of the gradient and $\nabla_{\theta}\mathcal{L}(\theta)$ denotes the gradient 
of the loss function with respect to parameters $\theta$. To compute the entire gradient, all the errors for the 
entire dataset would need to be averaged for the most accurate estimation of the gradient. Therefore variants 
like stochastic gradient descent exist, that first randomizes the order of dataset, second organizes it into batches of
the same size. Now we only average the gradient for all samples inside the batch and make an update to the model parameters \cite{Bottou10}.


To actually calculating the gradient the backpropagation algorithm is used. First we take out input data and do the
forward pass to compute the predictions. We can then calculate the loss by comparing the prediction to the desired result.
Now the idea behind backpropagation is to calculate the influence of the change weights and biases to the change in loss \cite{Rumelhart86}.
Mathematically this is represented by:


$$
\frac{\partial \mathcal{L}}{\partial \mathbf{W}^{(l)}} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}^{(l)}} \frac{\partial \mathbf{y}^{(l)}}{\partial \mathbf{W}^{(l)}}
$$


where $\mathcal{L}$ denotes the loss, $\mathbf{W}^{(l)}$ represents the weight matrix at layer $l$ and $\mathbf{y}^{(l)}$ stands for the output matrix
for layer $l$. Now this can be calculated at the output layer $L$ to get the error signal for the layer $L-1$. 
Then the same computation can be applied to the layer $L-1$ to propagate the error back.

\subsection{Recurrent Neural Networks}

Classic feed forward neural networks are only able to handle fix sized data, because the size of the input layer
is fixed. Sequences of variable length, like timeseries, audio or language data require a different
architecture to still being able to compute the gradients properly. This can be achieved by recurrent
neural networks (RNNs). Recurrent neural networks accomplish that by carrying a hidden state across timesteps.

\begin{center}
  \begin{tikzpicture}[>=latex,
    % styles for clarity
    input/.style   ={draw, circle, minimum size=8mm},
    hidden/.style  ={draw, rectangle, rounded corners=2pt, minimum size=8mm},
    output/.style  ={draw, circle, minimum size=8mm},
    node distance = 2cm
  ]
    \node[input](x) at (0,0){x};
    \node[hidden](h) at (2,0) {h};
    \node[output](o) at (4, 0) {o};

    \draw[->] (x) -- node[above]{$W_h$} (h);
    \draw[->] (h) -- node[above]{$W_y$} (o);
    \path
      (h) edge[->,
            out=-30,
            in=-150,
            looseness=7,
            loop]
            node[below]{$U_h$}
      (h);
  \end{tikzpicture}
  \captionof{figure}{Simple RNN with one hidden layer} 
  \label{fig:rnn}
\end{center}

The RNN Cell is very similar to the Perceptron. It weights, biases and an activation function. Additionally 
it has a recurrent connection to itself: the output of a rnn cell at timestep $t$ is feed back to itself at timestep
$t+1$ multiplied by its own weight. That output value is called the hidden state $h_t$ of an rnn cell.
Figure \ref{fig:rnn} depicts a RNN cell with one hidden layer.
A Elman RNN is described by the forward state update and output projection \cite{Elman90}:

$$
h_t = \phi\!\bigl(W_h x_t + U_h h_{t-1} + b_h\bigr), \quad
y_t = \psi\!\bigl(W_y h_t + b_y\bigr).
$$
which can be understood as a neural network with fully connected input and output layers together with a hidden rnn layer.
The forward state update projection $h_t$ is similar to the perceptron where $b_h$ represents the bias, $W_h$ the weights,
with the addition of the term $U_hh_{t-1}$. $h_{t-1}$ is the hidden state of the last iteration and $U_h$ is the weight
matrix for the hidden state. Typically that output is then feed though a fully connected output layer map the
potentially high dimensional hidden state into the output space.\\


To calculate the gradient in a network with recurrent connections you need to unroll the network in time. The network
is unrolled over a sequence of length $T$, by creating $T$ copies of the same cell. All copies share the same weights.
The recurrent connections are now longer connected to themselves but their copy in the next copy. Unrolling converts 
the cyclic graph of a RNN into an acyclic graph, which is required for applying the standard backpropagation algorithm.
Figure~\ref{fig:rnn_unroll} depicts the unrolled network for $T=3$.

\begin{center}
  \begin{tikzpicture}[>=latex,
      % styles for clarity
      input/.style   ={draw, circle, minimum size=8mm},
      hidden/.style  ={draw, rectangle, rounded corners=2pt, minimum size=8mm},
      output/.style  ={draw, circle, minimum size=8mm},
      node distance = 2cm
    ]
    \foreach \t [count=\i from 1] in {0,1,2}{
      % place nodes at (0,-2*\i), (2,-2*\i), (4,-2*\i)
      \node[input]  (x\i) at (0,-2*\i) {$x_{\t}$};
      \node[hidden] (h\i) at (2,-2*\i) {$h_{\t}$};
      \node[output] (o\i) at (4,-2*\i) {$o_{\t}$};

      % input → hidden and hidden → output
      \draw[->] (x\i) -- node[above] {$W_h$} (h\i);
      \draw[->] (h\i) -- node[above] {$W_y$} (o\i);

      % hidden_{t–1} → hidden_t for t>0
      \ifnum\i>1
        \pgfmathtruncatemacro{\j}{\i-1}
        \draw[->, out=-20, in=160, looseness=1.5]
          (h\j) to node[below] {$U_h$} (h\i);
      \fi
    }
  \end{tikzpicture}
  \captionof{figure}{Unrolled Elman RNN for $T=3$.}\label{fig:rnn_unroll}
\end{center}


Applying backpropagation on an unrolled graph is called backpropagation through time (BPTT). On the acyclic graph 
the standard backpropagation algorithm can be applied, but the weights for all instances of the recurrent neural network
in the unrolled graph are shared. The final update for the parameter matrices are accumulated though time steps, so there 
is a contribution from everywhere a weight is used \cite{werb1990bptt}.


\subsection{Limitations of Recurrent Neural Networks}

Classic recurrent neural networks suffer from some limitations, which motivated the development 
of other RNN Variants notably LSTM networks.

\begin{enumerate}[label=(\roman*)]
\item \textbf{Vanishing and exploding gradients.}  During BPTT the gradient with respect to earlier timesteps involves products of Jacobian
  matrices $\partial h_t/\partial h_{t-1}$.  For many activation functions this product tends to shrink or blow up exponentially with
  the number of timesteps, causing the gradient norm to \emph{vanish} or \emph{explode}.
  As a consequence the network either fails to learn long‑range dependencies or becomes numerically unstable~\cite{hochreiter1997lstm,pascanu2013rnntraining}.
  Exploding gradients can be taken care of by clipping the gradient to a maximum value \cite{pascanu2013rnntraining}.
  Vanish gradients though cannot be mitigated this easily. The vanishing gradients problem is the main motivation behind the
  development of LSTM networks.
\item \textbf{Limited memory capacity.}  The entire past of the sequence must be compressed into a single hidden vector $h_t$.
  In practice the dimensionality of $h_t$ is bounded by computational constraints, creating a tight information bottleneck.
  Empirical analyses show that standard RNNs spontaneously forget information that is not constantly reinforced.
\item \textbf{Difficulty modelling very long sequences.}  Even when exploding gradients are clipped and vanishing 
  gradients mitigated, the truncation window in BPTT means the effective context length is often limited to a few hundred
  time steps.  Tasks such as document‑level language modelling require mechanisms that can propagate information across thousands of steps \cite{pascanu2013rnntraining}.
\item \textbf{Bias towards recent inputs.}  The recurrent update $h_t=\phi(W_h x_t + U_h h_{t-1})$ blends the new input with a 
  transformed version of the previous hidden state.  Without gating, this blend is rigid and the network cannot dynamically decide 
  to \emph{copy} or \emph{forget} information.  As a result, RNNs tend to over‑weight recent inputs and under‑weight older ones.
\end{enumerate}

\section{LSTM Architecture (Fynn)}

This Section focuses on the Principle of Operation of the LSTM architecture, first trying to give an intuition to the operation of the architecture. Afterwards Networ

\subsection{Principle of Operation (Fynn)}
This Section first explains some prepositional concepts that make up the LSTM Architecture such as the Constant Error Carousel and Gate Units, after which the construction and operation of the architecture is explained.

\subsubsection{Constant Error Carousel (Fynn)}
Considering only a single neuron $j$, it is possible to archive Constant Error Flow through time, by fixing the weight of its feedback connection $w_{jj}=1$ and choosing the linear activation function $f_j(x)=x\ \forall x$. This construction, shown in Figure \ref{fig:CEC}, is called a Constant Error Carousel (CEC).\cite{hochreiter1997lstm}
\begin{center}
\begin{tikzpicture}
\node[circle,draw] (unit_j) {$f_j(x)=x$};
\draw[->] (unit_j) edge[loop above] node {$w_{jj}=1$} (unit_j);
\draw[->] (-3,0) -- node[above] {$w_{ji}$} (unit_j.west);
\draw[->] (unit_j.east) -- node[above] {$w_{jk}$} (3,0);
\end{tikzpicture}
\captionof{figure}{Single Neuron Constant Error Carousel\\Source: Based on \cite{hochreiter1997lstm}} 
\label{fig:CEC}
\end{center}
This works as the Error Backflow gets scaled by $f'_j(net_j(t))w_{jj}$ (where $net_j(t)$ is the activation of the neuron $j$ at time $t$) at every time step, which is why ensuring this expression remains fixed at $1$ mitigates exploding and vanishing gradients. As $f'(x)=1$ this is the case for arbitrary $net_j(t)$ with the chosen activation function and weight.\cite{hochreiter1997lstm} \\
Although this in theory fixes the problem of exploding and vanishing gradients in Recurrent Neural Networks, it leads to new problems in practice that make this solution impractical without further modifications. Consider some other neuron $i$ with activation $y^i(t)$ which is connected to $j$ with the weight $w_{ji}$.  Now For each time step, $y^i(t)w_{ji}$ gets added to the activation of $j$ at the previous time step. This makes the CEC a sort of memory, storing the previous inputs of the sequence by adding them together.\cite{hochreiter1997lstm} \\
But as $w_{ji}$ is the only way to control if the activations of $i$ should be stored in memory, it often receives conflicting weight update signals during training, making the training unstable. Similarly the output from the CEC's memory to another neuron $k$ is only controlled with a single weight $w_{jk}$, which also leads to unstable training. Further the storing and retrieving of the memory of $j$ often depends on the context, therefore a context sensitive mechanism is needed to control the access to the CEC's memory.\cite{hochreiter1997lstm}

\subsubsection{Gate Units (Fynn)}
The solution to the this problem proposed by Hochreiter and Schmidhuber is the so-called 'gate-unit'.\cite{hochreiter1997lstm} \\
Gate units allow to restrict the flow of information to and from a neuron, based on the context.\\
They work by scaling an input by multiplying it with a number computed from the context. This scaling factor is computed like with a regular neuron, where inputs from other neurons are added in a weighted sum and the result is passed through an activation function.\cite{hochreiter1997lstm}

\subsubsection{Memory Cell (Fynn)}
Adding a Gate Unit to the input and output of an CEC in combination with the functions $g$ for scaling the input to the and the function $h$ for scaling the output, results in a structure called a Memory Cell or LSTM Cell, as shown in Figure \ref{fig:lstm-cell}. They are the building block of LSTM Networks, and replace conventional neurons.\cite{hochreiter1997lstm} \\
The gate unit on the input side of the cell is often called input gate or update gate and the gate on the output side is called the output gate.\cite{hochreiter1997lstm} \\
\begin{figure}[h!]
    \centering
    \includegraphics[width=\linewidth]{LSTM-Cell-Diagram.png}
    \caption{LSTM Cell\\ Source: \cite{hochreiter1997lstm}}
    \label{fig:lstm-cell}
\end{figure}
The circles in Figure \ref{fig:lstm-cell}, represent neurons, with the arrows pointing in to them representing the inputs scaled by the according weights, the black dots represent multiplication.\cite{hochreiter1997lstm} \\
The arrows in Figure \ref{fig:lstm-cell} pointing into the cell from the bottom are the inputs for the gate units. For each gate all inputs are scaled by their respective weights and summed together. The result is then passed through an activation function to get the gate activations $y^{in_j}$ and $y^{out_j}$, which are the values used to control the access to the cells memory.\cite{hochreiter1997lstm} \\
On the left of Figure \ref{fig:lstm-cell} the cells input $net_{c_j}$ is computed in the same way as is done for the gate units, by summing the weighted inputs. This input then first gets scaled by the function $g$ and then scaled again by the input gates activation $y^{in_j}$ after which it reaches the cells internal CEC. \cite{hochreiter1997lstm} \\
Here the new input gets added to the previous activation of the CEC, to update the cells internal memory, also referred to as the cells internal state.\cite{hochreiter1997lstm} \\
Finally to generate an output from the cells memory, the output of the internal CEC gets first scaled by a function $h$ and then scaled again by the activation of the output gate $y^{out_j}$, in the same way as is done for the input gate. This results in the cell output activation $y^{c_j}$.\cite{hochreiter1997lstm} 

\subsection{Network Topology (Fynn)}

In Hochreiters and Schmidhubers paper, the specific inputs for the cell and gates are not defined as they depend on the chosen Network topology. Possible inputs could be the outputs of input units, memory cells, gate units or conventional hidden units.\cite{hochreiter1997lstm}\\
Most commonly used implementations of the LSTM Architecture, like the ones used included in machine learning librarys such as PyTorch or TensorFlow connect all the outputs of all cells, back to all the inputs and gate inputs of all the cells and biases to each input and gate input of each cell, as well as connecting other inputs, that do not depend on the state of the cells, to all inputs and gate inputs of all cells. \cite{smagulova2019notation, keras-lstm, real-pytorch-lstm, pytorch-lstm}

\subsection{Mathematical Formalization (Fynn)}

This Section focuses on two ways of formalizing the architecture, the first being the original original formalization by Hochreiter and Schmidhuber, the second being a modern matrix formulation for the most commonly used network topology.

\subsubsection{Single Memory Cell (Fynn)}
For a single memory cell $c_j$ the cell net input can be defined as:\cite{hochreiter1997lstm}
$$net_{c_j}(t)=\sum_u w_{c_ju}y^u (t-1)$$
The input gate net input as:\cite{hochreiter1997lstm}
$$net_{in_j}(t)=\sum_u w_{in_j u}y^u(t-1)$$
And the output gate net input as:\cite{hochreiter1997lstm}
$$net_{out_j}(t)=\sum_uw_{out_j u}y^u(t-1)$$
The summation index $u$ represents any units connected to the cell and its values depend on the networks topology.\cite{hochreiter1997lstm} \\
The are the result of passing the gate net inputs through their respective activation functions $f_{in_j}$ and $f_{out_j}$:\cite{hochreiter1991}
$$y^{in_j}(t)=f_{in_j}\left(net_{in_j}(t)\right)$$
and
$$y^{out_j}(t)=f_{out_j}\left(net_{out_j}(t)\right)$$
As the internal state always adds the new input scaled by the input gate to the previous state it can be defined as:\cite{hochreiter1997lstm}
$$s_{c_j}(0)=0$$
and
$$s_{c_j}(t)=s_{c_j}(t-1)+y_{in_j}(t)g\left(net_{c_j}(t)\right)\ \text{ for } t>0$$
Using this, the cells output can be calculated by scaling the cell state using the output gate activation:\cite{hochreiter1997lstm}
$$y^{c_j}(t)=y^{out_j}(t)h\left(s_{c_j}(t)\right)$$
\subsubsection{Block of Memory Cells (Fynn)}
For the most commonly implemented network topology, where the cells outputs are fully connected to their inputs and gate inputs, as well as some additional inputs, which is also fully connected to the cell inputs and gate inputs, as well as all inputs having a bias, the block can be described using matrix notation:\cite{smagulova2019notation}
\\
The activation of all input gates can be described by:\cite{smagulova2019notation}
$$i_t=\sigma\left(W^{(i)}x_t+U^{(i)}h_{t-1}+b^{(i)}\right)$$
Similarly the output gates activations are defined by:\cite{smagulova2019notation}
$$o_t=\sigma\left(W^{(o)}x_t+U^{(o)}h_{t-1}+b^{(o)}\right)$$
And the inputs of the cells, defined by:\cite{smagulova2019notation}
$$g_t=g\left(W^{(g)}x_t+U^{(g)}h_{t-1}+b^{(g)}\right)$$
The state for all cells can be described by:\cite{smagulova2019notation, hochreiter1991lstm}
$$C_t=g_t\odot i_t+C_{t-1}$$
From this the outputs of the cells can be defined as:\cite{smagulova2019notation, hochreiter1997lstm}
$$h_t=o_t\odot h(C_t)$$
The $\odot$ Operator is called the Hadamard product, which is defined as component wise multiplication of two vectors, which allows each cells input and output to be gated individually.\cite{gokmen2018hadamard, smagulova2019notation} \\ 

The function $\sigma$ (or $f$  is the sigmoid function defined as: \cite{hochreiter1997lstm} $$\sigma(x)=\frac{1}{1+\exp(-x)}$$
For the experiments in the original papers the functions $g$ and $h$ were defined as:\cite{hochreiter1997lstm}
$$g(x)=\frac{4}{1+\exp(-x)}-2$$
and \cite{hochreiter1997lstm}
$$h(x)=\frac{2}{1+\exp(-x)}-1$$
Today the $\tanh$ function is commonly used for the functions $g$ and $h$.\cite{smagulova2019notation}

\subsection{Results (Fynn)}

In their 1997 paper, Schmidhuber and Hochreiter compared the LSTM Architecture to other architectures for sequenced data, such as standard RNNs, trained with either Back Propagation Through Time (BPTT) or Real Time Recurrent Learning (RTRL), Elman nets, trained using the Elman's procedure, Recurrent Cascade-Correlation and neural sequencing chunking.\cite{hochreiter1997lstm}  \\

In their first experiment they trained their networks on an embedded reber gammer. As shown in Figure \ref{fig:exp1res}, while BPTT and RTRL did not always learn the gammer, the LSTM architecture always managed to learn the task. Further it also managed to learn the task much faster than the other architectures.\cite{hochreiter1997lstm}
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{ResultsTask1.png}
    \caption{Results of training the models on the embeded rber grammer, showing the percantage of successful trials, as well as the number of examples presented before success. The architectures compared in this table are Real Time Recurent Learning (RTRL), Elman net (ELM), Recurrent Cascade-Corellation (RCC) and LSTM\\Source: Taken from \cite{hochreiter1997lstm}}
    \label{fig:exp1res}
\end{figure}

The different architectures where also tested on noisy and noiseless sequences with long time lag.
For this input symbols where encoded using one-hot encoding.\cite{hochreiter1997lstm, qiu2022onehot}
\\ For the noise free sequences the models where trained on two similar sequences, one taking the form $(x, a_1, ..., a_n, x)$ the other the form $(y,a_1,...,a_n,y)$, so the model has to store the first token for $n$ time steps to be able to correctly predict the final token of the sequence.\cite{hochreiter1997lstm}\\
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{restask2.png}
    \caption{Results of training the models on sequences with different time lags, showing the percent of successful trials as well as the number of training examples till presented to the model until it learned the task. The architectures compared in this table are Real Time Recurrent Learning (RTRL), Back Propagation Through Time (BPTT) and neural sequence chunking (CH). \\Source: Taken from \cite{hochreiter1997lstm}}
    \label{fig:exp2res}
\end{figure}
As shown in Figure \ref{fig:exp2res}For short time lags like $n=4$ RTRL and BPTT are able to learn the task in some of the runs, with $n=10$ they always fail to learn the task. Neural sequence chunking was able to learn the task in some runs even with time lags as large as $n=100$. The LSTM architecture was again able to learn the task every time and again learned much faster than the other methods.\cite{hochreiter1997lstm}\\

As in this experiment the chunker sometimes learned to predict the local regularities of the sequence, another experiment was run, where the subsequence $(a_1, ..., a_n)$ was replaced by a random sequence of length $n$. RTRL, BPTT, and the chunker all failed to learn the task, while LSTM was able to learn it every time.\cite{hochreiter1997lstm}\\
\\
Another test was classification of seqeunces containing noise. For this sequences where generated where only the first $N$ elements gave information while the rest where noise and another test where noise was added to the data conveying information. In both cases the LSTM architecture could learn the given task.\cite{hochreiter1997lstm}
\\
Another difficult task the LSTM architecture was able to solve was the adding problem.\cite{hochreiter1997lstm}\\
Here sequences consist of pairs of numbers where the first is a real number between $-1.0$ and $1.0$ and the following number is $1.0$, $0.0$ or $-1.0$. The goal is to compute the sum of the first numbers in pairs, of which the second number is $1.0$. As with the other problem the LSTM architecture is able to learn the task and was able to process most testing sequences correctly . \cite{hochreiter1997lstm}\\
In a similar task, where the network had to multiply the two marked numbers, where the LSTM architecture again was able to learn the task. \cite{hochreiter1997lstm}
\section{Limitations and Alternatives (Fynn)}

This Section will first review some of the limitations of LSTM Netoworks, afterwards Variations of the architecture will be discussed, that try to alleviate some of those issues. Further the LSTM Architecture will be compared to the Transformer Architecture, which has been replacing LSTMs in many applications.

\subsection{Limitations (Fynn)}
Although LSTMs solve some of the problems associated with classical RNNs, they also have some disadvantages, including:
\begin{itemize}
    \item \textbf{Computational Intensity:} Computational intensity: As an LSTM cell requires additional weights for the gates, the number of weights can increase by up to a factor of \begin{math} 9 \end{math}. However, this is usually not a significant issue, as LSTM networks typically have a similar number of weights to other RNNs.\cite{hochreiter1997lstm}\\ However, in applications that require low power, such as mobile devices, this could cause problems. \cite{rizakisFPGAlstm}
    \item \textbf{Exploding Gradients:} Although the LSTM architecture mitigates the issue of vanishing gradients, it does not eliminate that of exploding gradients.\cite{pascanu2013rnntraining} This can be achieved using alternative techniques, such as gradient clipping.\cite{gradient-clipping, pascanu2013rnntraining}
    \item \textbf{Limited GPU utilization and memory bottlenecks:} LSTM networks do not utilize the GPU fully, as they are sequential in nature and can't be easily parallelized, which makes training less efficiently on modern GPUs. Furthermore, they are often limited by the available GPU memory.\cite{zheng2018scalability, zheng2020scalability}  
    \item \textbf{Worse performance on less complex sequences:} Although LSTMs perform better than some other architectures, such as GRUs, on sequences with high complexity, they struggle more on simpler sequences.\cite{cahuantzi2023lstmvsgru} 
    \item \textbf{Worse performance on short sequences:} LSTMs tend to perform poorly with short sequences, often having significantly higher losses. \cite{bolboaca2023lstmperformance} 
        \item \textbf{Over fitting:} Especially when trained on small data sets, LSTMs tend to over fit, which results from the large amount of parameters in a model. \cite{kinoyama2021overfitting}
        \item \textbf{Sensitivity to Hyper Parameters:} The performance of LSTM Networks can be significantly affected by hyper parameters, such as the batch size or learning rate. Selecting the right hyper parameters so that an LSTM Network works efficiently can require loads of trial and error. \cite{hochreiter1997lstm}
        \item \textbf{Interpretability:} The method with which LSTM Networks make their prediction is hard to interpret. This limits the extend to which LSTM Networks can be used in industries where explainable models are needed. \cite{firouzjaee2024interpret}
\end{itemize}

\subsection{Variants (Fynn)}

Many Variations of the LSTM Architecture have been developed to alleviate some of their Limitations. \\
This Section will give an overview of some important Variations that exist, such as the addition of a forget gate or Bidirectional LSTMs.

\subsubsection{LSTM with Forget Gate (Fynn)}
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.75\linewidth]{Structure-of-LSTM-cell-which-introduces-three-special-gates-Input-Gate-i-Forget-Gate.png}
    \caption{LSTM Cell with forget gate\\ Source: \cite{nguyen2022lstmforgetgraphic}}
    \label{fig:lstm-forget}
\end{figure}
One of the first variation of Hochreiter and Schmidhuber's architecture, was the introduction of a forget gate. The reason for its introduction is that there is no way for the internal state of the cell to be reset in the original architecture. To fix this a forget gate can be introduced, which restricts how much of the previous state is kept in the next time step.\cite{gers1999forgetgate}\\
Figure \ref{fig:lstm-forget} shows an LSTM cell, with the added forget gate on the left of the cell. Note that the input gate is labeled as "Update Gate" in this graphic.\\
The forget gate is implemented like the other with\cite{gers1999forgetgate}
\begin{equation}
    net_\varphi(t)=\sum_uw_{\varphi\ u}y^u(t-1)
    \label{eqn:forget-net-in}
\end{equation}
and
\begin{equation}
    y^\varphi(t)=f_\varphi\left(net_\varphi(t)\right).
    \label{eqn:forget-actiavtions}
\end{equation} 
Equation \ref{eqn:forget-net-in} describes the net inputs for the forget gate, and equation \ref{eqn:forget-actiavtions} describes the activation for the forget gate. Using those a new equation for the cell state can be defined as\cite{gers1999forgetgate}
\begin{equation}
    s_c(t)=y^\varphi(t)s_c(t-1)+y^{in}(t)g\left(net_c(t)\right).    
\end{equation}
The forget gate can also be defined using the matrix notation, where its activations are given by:\cite{smagulova2019notation}
$$f_t=\sigma\left(W^{(f)}x_t+U^{(f)}h_{t-1}+b^{(f)}\right)$$
and the internal state's of the cells can be defined as:\cite{smagulova2019notation}
$$C_t=g_t\odot i_t+f_t\odot C_{t-1}$$

The addition of an forget gate is very common, and many LSTM implementations in commonly used machine learning frameworks include a forget gate.\cite{smagulova2019notation,keras-lstm, real-pytorch-lstm, pytorch-lstm} 

\subsubsection{Bidirectional LSTM (Leon)}

\begin{figure}[h!]
    \centering
    \includegraphics[height=7.5cm]{brnn.png}
    \caption{Unrolled bidirectional recurrent network\\ Source: \cite{graves2013speechrecognitiondeeprecurrent}}
    \label{fig:brnn}
\end{figure}

Building upon bidirectional RNN \cite{Schuster97} the bidirectional LSTM processes input sequences not only in the 
forward direction but also in the backward direction. It has 2 lstm layers where one is processing the forward direction
and the other the backward direction. The final representation consists of the outputs of the 2 layers which are then merged together,
usually concatenation is used. In the final encoding both information about the past and the future are embedded \cite{Graves05}.\\
Figure \ref{fig:brnn} shows a bidirectional rnn with 2 recurrent layers. The first layer $\overrightarrow{h}_t$ processes
the input in the forward direction while the second layer $\overleftarrow{h}_t$ in the backward direction.



This architecture is used in sequence labeling \cite{huang2015bidirectionallstmcrfmodelssequence} and speech recognition \cite{graves2013speechrecognitiondeeprecurrent}.

\subsection{LSTM vs Transformers (Leon)}

The Transformer architecture relies on the attention mechanism as introduced in “Attention Is All You Need” paper.
The main difference is that the recurrent update used by LSTM networks is replaced by a self-attention mechanism \cite{vaswani2017attention}.\\


First, each input sequence is mapped to learned token embeddings and positional encodings. Positional encoding 
inject order information about where a token is inside of a sequence of tokens inside its token representation instead of in the architecture of the model as in RNNs.


The next innovation sparked by transformers was self-attention. Self-attention lets the model assign learned weights to the influence of every token on every other token.
When predicting the next token in sequence some tokens capture more semantic meaning than others and theise will be highlighted for the neural network by the 
attention mechanism. The attention is learned by the transformer in the training process \cite{vaswani2017attention}.
The computational and memory cost of self-attention as presentend in \cite{vaswani2017attention} is $\mathcal{O}(n^2)$. Though there are efforts
to optimize the self attention mechanism to reduce its complexity to $\mathcal{O}(n)$. \cite{katharopoulos2020transformersrnnsfastautoregressive}


While LSTM networks propagate past information by passsing the hidden state into the next iteration, transformers attend to the entire input sequence in every
step. Transformers do not suffer from vanishing or exploding gradient issues like RNN do.


Because self-attention removes any timestep-to-timestep dependency, a Transformer can process all tokens in a sequence simultaneously. 
In the original paper, Vaswani et al. report training their English-German “base” model to state-of-the-art quality in roughly 
12 hours on 8 P100 GPUs—several-times faster than earlier RNN/LSTM systems that must handle tokens one-by-one \cite{vaswani2017attention}.


Both architectures have their own advantages and disadvantages. LSTM networks are great at handling sequential data and 
remembering long term dependencies. Transformers are best at processing the data in parallel.


\section{Applications (Fynn)}

LSTM Networks have been used for a variety of applications through out the years.\\
This Section will highlight some of the most important applications of LSTM Networks, and provide some specific examples of how LSTMs where used in those areas.

\subsection{Speech Recognition (Leon)}

We have already established that LSTM networks are great at working with sequence data. One example where LSTM networks
have been successfully used is speech recognition \cite{Graves04}. The LSTM network learns temporal patterns in audio and captures long 
term relationships. They have achieved state of the art results in a paper by Google in 2014 \cite{sak2014longshorttermmemorybased}.
Researchers at google showed that a deep LSTM network outperformed traditional deep neural networks for this problem.
It was even better then a feed forward neural network with 10x more parameters \cite{sak2014longshorttermmemorybased}.\\


They used an LSTM variant called Long Short-Term Memory Projected (LSTMP) architecture \cite{sak2014longshorttermmemorybased} which
outperformed the traditional LSTM architecture. The LSTMP architecture differs from the traditional LSTM one by not having a LSTM layer
which output is directly connected to its input but they first do a linear projection with a fully connected layer and its output will represent the 
hidden state of the cell.

\subsection{Time Series Forecasting (Fynn)}
While LSTMs can be used to forecast time series, they often are inferior to other methods.\cite{gers2001timeseries} But for certain applications, like speech or music, they are a better choice, as they perform better on more complex sequences.\cite{gers2001timeseries, cahuantzi2023lstmvsgru}\\
In a 2022 article by Torres, they use an deep LSTM network, to predict electricity demands and achieve errors of just \begin{math}1.5\%\end{math}.\cite{torres2022elctricityforecasting}\\
Kılıç et al. (2024) uses the LSTM architecture to predict electricity prices, and achieve better results compared to other architectures.\cite{nielsen2024electricitypriceforcasting}

\subsection{Music and Audio (Fynn)}
Music has structures and patterns, where the next note or chord depends on the previous ones. LSTMs can learn those patterns and predict what comes next, which allows them to generate music.\cite{eck2002musicgeneration}\\
In their 2002 paper Eck and Schmidhuber use an LSTM network to generate chord progressions, as well as melodies and chord progressions. Their network successfully managed to pick up on patterns in a style of music and reproduce those patterns to compose new music in the same style.\cite{eck2002musicgeneration}  

\subsection{Medical (Leon)}

A 7 layer LSTM network was trained to detect Type 2 diabetes in the early stages in Oman. The model is composed of five stacked
LSTM layeres with 20 Units each feeding into a fully connected output layer. It reaches \begin{math}99.4 \%\end{math} accuracy and an 
AUROC of \begin{math}0.945\end{math} while maintaining \begin{math}100 \%\end{math} sensitivity on a \begin{math}20 \%\end{math} hold-out set \cite{AlSadi2024LSTM}.
These results surpass the logistic regression baseline.


\section{Conclusion (Fynn)}
The LSTM architecture introduced gate units and memory cells, effectively eliminating the vanishing gradient problem that had prevented RNNs from reaching their full potential.\cite{hochreiter1997lstm} 
This enabled LSTMs to be used for tasks such as time series forecasting, music generation, and natural language processing.\cite{eck2002musicgeneration,nielsen2024electricitypriceforcasting,gers2001timeseries,torres2022elctricityforecasting,sak2014longshorttermmemorybased}\\
Although LSTMs have been replaced by architectures such as transformers in many applications, they are still useful because they perform better with small datasets. This is why further research into improving the performance of this architecture is likely.\cite{alselwi2024lstmfuture} Furthermore, they will likely form the basis of new architectures. For example, LSTM-Transformer hybrid architectures already exist today. \cite{zhao2025lstmtransformerhybrid} 

\bibliographystyle{alpha}
\bibliography{abbrev,seminar_report}
\end{document}
